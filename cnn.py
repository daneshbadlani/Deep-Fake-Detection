# -*- coding: utf-8 -*-
"""CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Um-dH7NEx9OhdSG7KNVbeWLi7KPmOFap
"""



import gdown
from zipfile import ZipFile

import shutil
import imageio
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
import nltk
import tensorflow as tf
import tensorflow.keras
from tensorflow.keras import Sequential, Model
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import (
    LSTM,
    Dense,
    Embedding,
    Dropout,
    SpatialDropout1D,
    Activation,
    Input,
    Reshape,
    BatchNormalization,
    ReLU,
    Conv1D,
    Flatten,
    AveragePooling1D,
    LeakyReLU,
)
from tensorflow.keras.losses import (
    BinaryCrossentropy
)
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import RMSprop
import os
from PIL import Image
from tensorflow.python.keras.layers.convolutional import Conv2D, Conv2DTranspose
from tqdm import tqdm
from IPython import display
import time



"""IMAGE PREPROCESSING"""
PIC_DIR = './celeba_gan/img_align_celeba/'
IMAGES_COUNT = 10000
ORIG_WIDTH = 178
ORIG_HEIGHT = 208
diff = (ORIG_HEIGHT - ORIG_WIDTH) // 2
WIDTH = 128
HEIGHT = 128
crop_rect = (0, diff, ORIG_WIDTH, ORIG_HEIGHT - diff)
realImages = []
for imageName in tqdm(os.listdir(PIC_DIR)[IMAGES_COUNT:15000]):
    pic = Image.open(PIC_DIR + imageName).crop(crop_rect)
    pic.thumbnail((WIDTH, HEIGHT), Image.ANTIALIAS)
    realImages.append(np.uint8(pic))  # Normalize the images
realImages = np.array(realImages) / 255
print("Images Shape:", realImages.shape)
plt.figure(1, figsize=(10, 10))

# Saving sample 25 images to see what faces look like


"""IMAGE PREPROCESSING"""
PIC_DIR = './fakeImages30000/'
IMAGES_COUNT = 10000
ORIG_WIDTH = 178
ORIG_HEIGHT = 208
diff = (ORIG_HEIGHT - ORIG_WIDTH) // 2
WIDTH = 128
HEIGHT = 128
crop_rect = (0, diff, ORIG_WIDTH, ORIG_HEIGHT - diff)
images = []
for imageName in tqdm(os.listdir(PIC_DIR)[:5000]):
    pic = Image.open(PIC_DIR + imageName)
    pic.thumbnail((WIDTH, HEIGHT), Image.ANTIALIAS)
    images.append(np.uint8(pic))  # Normalize the images
images = np.array(images) / 255
print("Images Shape:", images.shape)
plt.figure(1, figsize=(10, 10))

# Saving sample 25 images to see what faces look like
for i in range(25):
    plt.subplot(5, 5, i+1)
    plt.imshow(images[i])
    # plt.axis('off')
plt.savefig('./test1')

"""
Author: Ana Estrada and Danesh Badlani
File: CNN_Assignment.py
"""


from keras.utils.np_utils import to_categorical   
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D,MaxPool2D,Dense,Flatten,Dropout,Input,Conv2D,MaxPooling2D,BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import optimizers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical
import numpy as np


## Load is Cifar 10 dataset
dataSet = np.concatenate([images, realImages], axis =0)
y_values = []
for i in range(0,5000):
  y_values.append(0)
for i in range(0, 5000):
  y_values.append(1)
X_train, X_test, y_train, y_test = train_test_split(dataSet, np.array(y_values), shuffle = True, test_size = 0.1 )#IMAGE DATA loading it in
y_train = to_categorical(y_train, 2)
y_test = to_categorical(y_test, 2)
## Print out shapes of training and testing sets
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

## Normalize x train and x test images
#transfer learning maybe 
##X_train, X_test = X_train / 255.0, X_test / 255.0
##
##init_model = tf.keras.applications.#name of the transfer learing model(input_shape + IMG_SHAPE, include_top = FALSE, weights = 'imagenet')
##
##feature_batch = init_model(image_batch)
##
##init_model.trainable = False

## Define the model
model = Sequential()

## Add a convolutional layer with 32 filters, 3x3 kernel, relu activation, he uniform kernel initializer, same padding and input shape
model.add(Conv2D(filters = 32, input_shape = (128, 128, 3), kernel_size = (3, 3), activation = 'relu', kernel_initializer = "he_uniform", padding = 'same'))

## Add a batch normalization layer
model.add(BatchNormalization())

## Add a convolutional layer with 32 filters, 3x3 kernel, relu activation, he uniform kernel initializer, and same padding 
model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation = 'relu', kernel_initializer = 'he_uniform', padding = 'same'))

## Add a batch normalization layer
model.add(BatchNormalization())

## Add a max pooling 2d layer with 2x2 size
model.add(MaxPooling2D(pool_size = (2,2)))

## Add dropout layer of 0.2
model.add(Dropout(rate = 0.2))

## Add a convolutional layer with 64 filters, 3x3 kernel, relu activation, he uniform kernel initializer, and same padding
model.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu', kernel_initializer = 'he_uniform', padding= 'same'))

## Add a batch normalization layer
model.add(BatchNormalization())

## Add a convolutional layer with 64 filters, 3x3 kernel, relu activation, he uniform kernel initializer, and same padding
model.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu', kernel_initializer = 'he_uniform', padding= 'same'))

## Add a batch normalization layer
model.add(BatchNormalization())

## Add a max pooling 2d layer with 2x2 size
model.add(MaxPooling2D(pool_size = (2,2)))

## Add dropout layer of 0.2
model.add(Dropout(rate = 0.2))

## Add a convolutional layer with 128 filters, 3x3 kernel, relu activation, he uniform kernel initializer, and same padding
model.add(Conv2D(filters = 128, kernel_size = (3, 3), activation = 'relu', kernel_initializer = 'he_uniform', padding= 'same'))

## Add a batch normalization layer
model.add(BatchNormalization())

## Add a convolutional layer with 128 filters, 3x3 kernel, relu activation, he uniform kernel initializer, and same padding
model.add(Conv2D(filters = 128, kernel_size = (3, 3), activation = 'relu', kernel_initializer = 'he_uniform', padding = 'same'))

## Add a batch normalization layer
model.add(BatchNormalization())

## Add a max pooling 2d layer with 2x2 size
model.add(MaxPooling2D(pool_size = (2,2)))

## Add dropout layer of 0.2
model.add(Dropout(rate = 0.2))

## Flatten the resulting data
model.add(Flatten())

## Add a dense layer with 128 nodes, relu activation and he uniform kernel initializer
model.add(Dense(128, activation = 'relu', kernel_initializer = 'he_uniform'))

## Add a batch normalization layer
model.add(BatchNormalization())

## Add dropout layer of 0.2
model.add(Dropout(rate = 0.2))

## Add a dense softmax layer
model.add(Dense(2, activation = 'softmax'))

## Set up early stop training with a patience of 3
stop = EarlyStopping(patience = 3)

## Compile the model with adam optimizer, categorical cross entropy and accuracy metrics
model.compile(optimizer = 'adam', loss ='binary_crossentropy', metrics = ['accuracy'])


## Fit the model with the generated data, 200 epochs, steps per epoch and validation data defined. 
print(X_train.shape)
print(y_train.shape)
history = model.fit(X_train, y_train, epochs = 25,  callbacks = stop, validation_split = 0.1)

results = model.evaluate(X_test, y_test)

"""
Author: Ana Estrada and Danesh Badlani
File: CNN_Assignment.py
"""


from keras.utils.np_utils import to_categorical   
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D,MaxPool2D,Dense,Flatten,Dropout,Input,Conv2D,MaxPooling2D,BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import optimizers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical
import numpy as np
"""IMAGE PREPROCESSING"""
PIC_DIR = './celeba_gan/img_align_celeba/'
IMAGES_COUNT = 10000
ORIG_WIDTH = 178
ORIG_HEIGHT = 208
diff = (ORIG_HEIGHT - ORIG_WIDTH) // 2
WIDTH = 128
HEIGHT = 128
crop_rect = (0, diff, ORIG_WIDTH, ORIG_HEIGHT - diff)
realImages = []
for imageName in tqdm(os.listdir(PIC_DIR)[IMAGES_COUNT:20000]):
    pic = Image.open(PIC_DIR + imageName).crop(crop_rect)
    pic.thumbnail((WIDTH, HEIGHT), Image.ANTIALIAS)
    realImages.append(np.uint8(pic))  # Normalize the images
realImages = np.array(realImages) / 255
print("Images Shape:", realImages.shape)
plt.figure(1, figsize=(10, 10))

# Saving sample 25 images to see what faces look like
for i in range(25):
    print("here")
    plt.subplot(5, 5, i+1)
    plt.imshow(realImages[i])
    # plt.axis('off')
plt.savefig('./test')

## Load is Cifar 10 dataset

print("here")
dataSet = np.concatenate([ realImages], axis =0)
y_values = []
for i in range(0,10000):
  y_values.append(0)
for i in range(10001, 20000):
  y_values.append(1) 
(X_train, y_train), (X_test, y_test) = train_test_split(dataSet, y_values, random_state = 0.42, test_size = 0.1 )#IMAGE DATA loading it in

## Print out shapes of training and testing sets
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

## Normalize x train and x test images
#transfer learning maybe 
##X_train, X_test = X_train / 255.0, X_test / 255.0
##
##init_model = tf.keras.applications.#name of the transfer learing model(input_shape + IMG_SHAPE, include_top = FALSE, weights = 'imagenet')
##
##feature_batch = init_model(image_batch)
##
##init_model.trainable = False

## Define the model
print("here")
model = Sequential()

## Add a convolutional layer with 32 filters, 3x3 kernel, relu activation, he uniform kernel initializer, same padding and input shape
model.add(Conv2D(filters = 32, input_shape = (218, 178, 3), kernel_size = (3, 3), activation = 'relu', kernel_initializer = "he_uniform", padding = 'same'))

## Add a batch normalization layer
model.add(BatchNormalization())

## Add a convolutional layer with 32 filters, 3x3 kernel, relu activation, he uniform kernel initializer, and same padding 
model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation = 'relu', kernel_initializer = 'he_uniform', padding = 'same'))

## Add a batch normalization layer
model.add(BatchNormalization())

## Add a max pooling 2d layer with 2x2 size
model.add(MaxPooling2D(pool_size = (2,2)))

## Add dropout layer of 0.2
model.add(Dropout(rate = 0.3))

## Add a convolutional layer with 64 filters, 3x3 kernel, relu activation, he uniform kernel initializer, and same padding
model.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu', kernel_initializer = 'he_uniform', padding= 'same'))

## Add a batch normalization layer
model.add(BatchNormalization())

## Add a convolutional layer with 64 filters, 3x3 kernel, relu activation, he uniform kernel initializer, and same padding
model.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu', kernel_initializer = 'he_uniform', padding= 'same'))

## Add a batch normalization layer
model.add(BatchNormalization())

## Add a max pooling 2d layer with 2x2 size
model.add(MaxPooling2D(pool_size = (2,2)))

## Add dropout layer of 0.2
model.add(Dropout(rate = 0.3))

## Add a convolutional layer with 128 filters, 3x3 kernel, relu activation, he uniform kernel initializer, and same padding
model.add(Conv2D(filters = 128, kernel_size = (3, 3), activation = 'relu', kernel_initializer = 'he_uniform', padding= 'same'))

## Add a batch normalization layer
model.add(BatchNormalization())

## Add a convolutional layer with 128 filters, 3x3 kernel, relu activation, he uniform kernel initializer, and same padding
model.add(Conv2D(filters = 128, kernel_size = (3, 3), activation = 'relu', kernel_initializer = 'he_uniform', padding = 'same'))

## Add a batch normalization layer
model.add(BatchNormalization())

## Add a max pooling 2d layer with 2x2 size
model.add(MaxPooling2D(pool_size = (2,2)))

## Add dropout layer of 0.2
model.add(Dropout(rate = 0.3))

## Flatten the resulting data
model.add(Flatten())

## Add a dense layer with 128 nodes, relu activation and he uniform kernel initializer
model.add(Dense(128, activation = 'relu', kernel_initializer = 'he_uniform'))

## Add a batch normalization layer
model.add(BatchNormalization())

## Add dropout layer of 0.2
model.add(Dropout(rate = 0.3))

## Add a dense softmax layer
model.add(Dense(2, activation = 'softmax'))

## Set up early stop training with a patience of 3
stop = EarlyStopping(patience = 3, delta = 0.00001)

## Compile the model with adam optimizer, categorical cross entropy and accuracy metrics
model.compile(optimizer = 'adam', loss ='binary_crossentropy', metrics = ['accuracy'])


## Fit the model with the generated data, 200 epochs, steps per epoch and validation data defined. 
history = model.fit(X_train, y_train, epochs = 5,  callbacks = stop, validation_split = 0.1)

results = model.evaluate(X_test, y_test)
